{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b46c978-1d90-488c-87a6-4c4aafca3258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import datasets\n",
    "from datasets import ClassLabel, load_dataset, Dataset, DatasetDict\n",
    "import string\n",
    "from typing import Dict ,List\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AdamW\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c6f73-5b10-4f85-8e87-e79746c3ef08",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197434b1-87c5-4482-af6c-78f257b61653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/gufran/Developer/Projects/AI/MawqifStanceDetection/data\"\n",
    "MODEL_PATH = \"/Users/gufran/Developer/Projects/AI/MawqifStanceDetection/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dedacaef-76a4-498b-b55f-915afa9bef5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02-twitter and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"aubmindlab/bert-base-arabertv02-twitter\"\n",
    "# model_name = \"aubmindlab/bert-base-arabertv02\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9340d89f-dfc3-412c-8e66-2106a254e546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784b50c-ee68-4d5d-bdec-76979a5b6b43",
   "metadata": {},
   "source": [
    "## DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9b48ab-6b6a-4452-aba8-d8e7681f157a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MENTION MENTION MENTION MENTION MENTION MENTIO...</td>\n",
       "      <td>No</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Against</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MENTION سؤالك غلط ! نعم اللقاح يمنع بصورة كبير...</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Favor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تمكين المرأه شي جميل  الغير جيد ان مرحلة تمكين...</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Favor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>شرطة عمان السلطانية تقدمت خطوات كبيره عن باقي ...</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Favor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>التحول الرقمى والتكنولوجيا الحديثة تنقذ \"الكهر...</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Favor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sarcasm sentiment  \\\n",
       "0  MENTION MENTION MENTION MENTION MENTION MENTIO...      No  Negative   \n",
       "1  MENTION سؤالك غلط ! نعم اللقاح يمنع بصورة كبير...      No  Positive   \n",
       "2  تمكين المرأه شي جميل  الغير جيد ان مرحلة تمكين...      No  Positive   \n",
       "3  شرطة عمان السلطانية تقدمت خطوات كبيره عن باقي ...      No  Positive   \n",
       "4  التحول الرقمى والتكنولوجيا الحديثة تنقذ \"الكهر...      No  Positive   \n",
       "\n",
       "    stance  \n",
       "0  Against  \n",
       "1    Favor  \n",
       "2    Favor  \n",
       "3    Favor  \n",
       "4    Favor  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"cleaned.csv\"))\n",
    "df = df.dropna(subset=[\"stance\"])\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df = df[[\"text\", \"sarcasm\", \"sentiment\", \"stance\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1a9a0e9-9122-454a-b4e7-2087af28eca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapping_sarcasm = {\"No\": 0, \"Yes\": 1}\n",
    "df['sarcasm'] = df['sarcasm'].map(lambda x: mapping_sarcasm[x])\n",
    "\n",
    "mapping_sentiment = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "df['sentiment'] = df['sentiment'].map(lambda x: mapping_sentiment[x])\n",
    "\n",
    "mapping_stance = {\"Favor\": 1, \"Against\": 0}\n",
    "df['stance'] = df['stance'].map(lambda x: mapping_stance[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829e4180-e0d2-4ea3-a3a6-43a76cb04eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_punctuations = '''`÷×؛<>()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "def remove_hash_URL_MEN(text):\n",
    "    text = re.sub(r'#',' ',text)\n",
    "    text = re.sub(r'_',' ',text)\n",
    "    text = re.sub(r'URL','',text)\n",
    "    text = re.sub(r'MENTION','',text)\n",
    "    return text\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إآ]\", \"ا\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def process_tweet(tweet):     \n",
    "    tweet=remove_hash_URL_MEN(tweet)\n",
    "    tweet = re.sub('@[^\\s]+', ' ', str(tweet))\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',str(tweet))    \n",
    "    tweet= normalize_arabic(str(tweet))\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "df.text = df.text.apply(lambda x: process_tweet(x))\n",
    "df.text = df.text.apply(lambda x: arabert_prep.preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ab11eb2-8320-45df-9eb7-7dc30774af85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ربئي يا ربئي ! يعنني بدوية الساس والمنشى تنعته...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>سؤالك غلط ! نعم اللقاح يمنع بصورة كبيره وبنسبة...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تمكين المرأه شي جميل الغير جيد ان مرحلة تمكين ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>شرطة عمان السلطانية تقدمت خطوات كبيره عن باقي ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>التحول الرقمى والتكنولوجيا الحديثة تنقذ \" الكه...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sarcasm  sentiment  \\\n",
       "0  ربئي يا ربئي ! يعنني بدوية الساس والمنشى تنعته...        0          0   \n",
       "1  سؤالك غلط ! نعم اللقاح يمنع بصورة كبيره وبنسبة...        0          2   \n",
       "2  تمكين المرأه شي جميل الغير جيد ان مرحلة تمكين ...        0          2   \n",
       "3  شرطة عمان السلطانية تقدمت خطوات كبيره عن باقي ...        0          2   \n",
       "4  التحول الرقمى والتكنولوجيا الحديثة تنقذ \" الكه...        0          2   \n",
       "\n",
       "   stance  \n",
       "0       0  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0c3be2d-9402-42ca-8df2-1a0d0d8076e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df[[\"text\"]]\n",
    "y = df[[\"sentiment\", \"sarcasm\", \"stance\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e8f5e7d-6b5b-42b1-9355-fd465d3f4e84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2693, 1) y_train shape: (2693, 3) X_test shape: (476, 1) y_test shape: (476, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape, \"X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dbb7423-ef7d-4ebd-8d80-cbf6cd933b36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataframe length divisible by batch_size: True\n",
      "Test dataframe length divisible by batch_size: True\n",
      "X_train shape: (2688, 1) y_train shape: (2688, 3) X_test shape: (448, 1) y_test shape: (448, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train_extra = len(X_train) % batch_size\n",
    "x_test_extra = len(X_test) % batch_size\n",
    "\n",
    "if x_train_extra > 0:\n",
    "    indices_to_drop = random.sample(X_train.index.tolist(), x_train_extra)\n",
    "    X_train = X_train.drop(indices_to_drop)\n",
    "    y_train = y_train.drop(indices_to_drop)\n",
    "    \n",
    "if x_test_extra > 0:\n",
    "    indices_to_drop = random.sample(X_test.index.tolist(), x_test_extra)\n",
    "    X_test = X_test.drop(indices_to_drop)\n",
    "    y_test = y_test.drop(indices_to_drop)\n",
    "\n",
    "print(\"Train dataframe length divisible by batch_size:\", len(X_train) % batch_size == 0)\n",
    "print(\"Test dataframe length divisible by batch_size:\", len(X_test) % batch_size == 0)\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape, \"X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1c209bc-c448-4e01-8e90-6797a90e4fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "    return tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "encoded_tweets_train = [encode_text(text) for text in X_train[\"text\"]]\n",
    "encoded_tweets_test = [encode_text(text) for text in X_test[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa2ce39f-6748-4971-986b-27a2f2b3392b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2688"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_tweets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9907e689-c3fd-4ab7-a94a-4a12901fb062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(encoded_tweets, labels):\n",
    "    len_encoded_tweets = len(encoded_tweets)\n",
    "    num_batches = len_encoded_tweets//batch_size\n",
    "    \n",
    "    for b in range(num_batches):\n",
    "        main_tweets, context_tweets, sentiments, sarcasms, stances, similarities = [], [], [], [], [], []\n",
    "        for i in range(batch_size*b, (batch_size*b) + batch_size):\n",
    "            context_tweet_index = random.randint(0, len_encoded_tweets-1)\n",
    "            main_tweets.append(encoded_tweets[i])\n",
    "            context_tweets.append(encoded_tweets[context_tweet_index])\n",
    "            \n",
    "            mt_sent = labels.sentiment.iloc[i]\n",
    "            mt_sarc = labels.sarcasm.iloc[i]\n",
    "            ct_sent = labels.sentiment.iloc[context_tweet_index]\n",
    "            ct_sarc = labels.sarcasm.iloc[context_tweet_index]\n",
    "            similarity = (int(mt_sent == ct_sent) + int(mt_sarc == ct_sarc))/2\n",
    "            \n",
    "            sentiments.append(labels.sentiment.iloc[i])\n",
    "            sarcasms.append(labels.sarcasm.iloc[i])\n",
    "            stances.append(labels.stance.iloc[i])\n",
    "            similarities.append(similarity)\n",
    "            \n",
    "        yield main_tweets, context_tweets, sentiments, sarcasms, stances, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61f5bcbf-eb8b-4d64-8431-8b7faff417db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sarcasm_labels = len(df.sarcasm.unique())\n",
    "num_sentiment_labels = len(df.sentiment.unique())\n",
    "num_stance_labels = len(df.stance.unique())\n",
    "\n",
    "num_sarcasm_labels, num_sentiment_labels, num_stance_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987a76c-5bc9-4967-ad1e-4525d02515ea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8436701-67d5-40f9-bf93-ebc3290befc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fb3f629-18d4-4bbb-b6d4-adb7fbdcd1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SubTaskHead(nn.Module):\n",
    "    def __init__(self, num_labels, dropout, hidden_size):\n",
    "        super(SubTaskHead, self).__init__()\n",
    "        \n",
    "        self.num_labels = num_labels\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        logits = self.dropout(inputs)\n",
    "        logits = self.classifier(logits)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "842d047b-4836-478c-99e9-331b04b87736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimilarityHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimilarityHead, self).__init__()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedding1, embedding2 = inputs\n",
    "\n",
    "        cosine_sim = F.cosine_similarity(embedding1, embedding2, dim=1)        \n",
    "        scaled_similarity = 0.5 * (cosine_sim + 1)\n",
    "        \n",
    "        return scaled_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a211e77d-4422-4806-b4eb-5184df5692d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StanceHead(torch.nn.Module):\n",
    "    def __init__(self, last_hidden_state_size, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(last_hidden_state_size, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.classifier = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, mt_last_hidden_state, sentiments, sarcasms, similarities):\n",
    "        node_features = torch.tensor(mt_last_hidden_state, dtype=torch.float)\n",
    "        \n",
    "        edges = []\n",
    "        edge_features = []\n",
    "        for i in range(len(sentiments)):\n",
    "            for j in range(i+1, len(sentiments)):\n",
    "                if sentiments[i] == sentiments[j] or sarcasms[i] == sarcasms[j]:\n",
    "                    edges.append((i, j))\n",
    "                    if sentiments[i] == sentiments[j] and sarcasms[i] == sarcasms[j]:\n",
    "                        edge_features.append(1.0)\n",
    "                    else:\n",
    "                        edge_features.append(0.5)\n",
    "        \n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "\n",
    "        data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        \n",
    "        data = data.to(device)\n",
    "        \n",
    "        x = self.conv1(data.x, data.edge_index, data.edge_attr)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, data.edge_index, data.edge_attr)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c772d61-1c37-47ff-812f-2272fefe2be7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, bert, sentiment_head, sarcasm_head, similarity_head, stance_head=None, hidden_layer_size = 512):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "        self.hidden_layer = nn.Linear(bert.config.hidden_size, hidden_layer_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.sentiment_head = sentiment_head\n",
    "        self.sarcasm_head = sarcasm_head\n",
    "        self.similarity_head = similarity_head\n",
    "        self.stance_head = stance_head\n",
    "        \n",
    "    def forward(self, mt_input_ids, mt_attention_mask, ct_input_ids, ct_attention_mask):\n",
    "        mt_outputs = self.bert(input_ids=mt_input_ids, attention_mask=mt_attention_mask)\n",
    "        ct_outputs = self.bert(input_ids=ct_input_ids, attention_mask=ct_attention_mask)\n",
    "                \n",
    "        mt_last_hidden_state = mt_outputs.last_hidden_state[:, 0, :]\n",
    "        ct_last_hidden_state = ct_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        hidden_output = self.dropout(F.relu(self.hidden_layer(mt_last_hidden_state)))\n",
    "        sarcasm_logits = self.sarcasm_head(hidden_output)\n",
    "        sentiment_logits = self.sentiment_head(hidden_output)\n",
    "        similarity_logits = self.similarity_head((mt_last_hidden_state, ct_last_hidden_state))\n",
    "        \n",
    "        sentiments = sentiment_logits.argmax(axis=1)\n",
    "        sarcasms = sarcasm_logits.argmax(axis=1)\n",
    "        stance_logits = self.stance_head(mt_last_hidden_state, sentiments, sarcasms, similarity_logits)\n",
    "        \n",
    "        return sarcasm_logits, sentiment_logits, similarity_logits, stance_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e077a-c20c-40b1-b374-a71249e27cf6",
   "metadata": {},
   "source": [
    "## TRAINING & VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "183394a0-43d4-40f0-b3e4-cfdfc13a130d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7665794f354c17843d790a39d31fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Sarcasm -> Loss: 0.007062323014473631, Acc: 0.9278273809523809\n",
      "Sentiment -> Loss: 0.030205026180261656, Acc: 0.5316220238095238\n",
      "Stance -> Loss: 0.024587264528409356, Acc: 0.5517113095238095\n",
      "Similarity -> Loss: 0.002976750051380978\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8cbd65bbf14d838eab96d2e81eddf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'sarcasm_criterion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 142\u001b[0m\n\u001b[1;32m    138\u001b[0m sim_y \u001b[38;5;241m=\u001b[39m sim_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    140\u001b[0m sarcasm_logits, sentiment_logits, similarity_logits, stance_logits \u001b[38;5;241m=\u001b[39m model(mt_input_ids, mt_attention_mask, ct_input_ids, ct_attention_mask)\n\u001b[0;32m--> 142\u001b[0m sarcasm_loss \u001b[38;5;241m=\u001b[39m sarcasm_criterion(sarcasm_logits, sarc_y)\n\u001b[1;32m    143\u001b[0m sentiment_loss \u001b[38;5;241m=\u001b[39m sentiment_criterion(sentiment_logits, sent_y)\n\u001b[1;32m    144\u001b[0m similarity_loss \u001b[38;5;241m=\u001b[39m similarity_criterion(similarity_logits\u001b[38;5;241m.\u001b[39msqueeze(), sim_y\u001b[38;5;241m.\u001b[39mfloat())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sarcasm_criterion' is not defined"
     ]
    }
   ],
   "source": [
    "subtask_hidden_layer_size = 512\n",
    "gnn_hidden_layer_size = 128\n",
    "bert_emb_size = bert.config.hidden_size\n",
    "\n",
    "model = MultiTaskModel(\n",
    "    bert=bert,\n",
    "    sentiment_head=SubTaskHead(num_sentiment_labels, 0.1, subtask_hidden_layer_size),\n",
    "    sarcasm_head=SubTaskHead(num_sarcasm_labels, 0.1, subtask_hidden_layer_size),\n",
    "    similarity_head=SimilarityHead(),\n",
    "    stance_head=StanceHead(bert_emb_size, gnn_hidden_layer_size, num_stance_labels)\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}-------------------------------------------\")\n",
    "    \n",
    "    # TRAINING -------------------------------------------------------------------------------\n",
    "    model.train()\n",
    "    train_loader_gen = data_generator(encoded_tweets_train, y_train)\n",
    "    valid_loader_gen = data_generator(encoded_tweets_test, y_test)\n",
    "    \n",
    "    epoch_sarcasm_loss = 0.0\n",
    "    epoch_sentiment_loss = 0.0\n",
    "    epoch_similarity_loss = 0.0\n",
    "    epoch_stance_loss = 0.0\n",
    "    \n",
    "    correct_sarcasm = 0\n",
    "    correct_sentiment = 0\n",
    "    correct_similarity = 0\n",
    "    correct_stance = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for main_tweets, context_tweets, sent_y, sarc_y, stance_y, sim_y in tqdm(train_loader_gen, desc=\"Training\"):\n",
    "        dataset = TensorDataset(\n",
    "            torch.cat([item[\"input_ids\"] for item in main_tweets]),\n",
    "            torch.cat([item[\"attention_mask\"] for item in main_tweets]),\n",
    "            torch.cat([item[\"input_ids\"] for item in context_tweets]),\n",
    "            torch.cat([item[\"attention_mask\"] for item in context_tweets]),\n",
    "            torch.tensor(sarc_y),\n",
    "            torch.tensor(sent_y),\n",
    "            torch.tensor(stance_y),\n",
    "            torch.tensor(sim_y),\n",
    "        )\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for batch in loader:\n",
    "            mt_input_ids, mt_attention_mask, ct_input_ids, ct_attention_mask, sarc_y, sent_y, stance_y, sim_y = batch\n",
    "            mt_input_ids = mt_input_ids.to(device)\n",
    "            mt_attention_mask = mt_attention_mask.to(device)\n",
    "            ct_input_ids = ct_input_ids.to(device)\n",
    "            ct_attention_mask = ct_attention_mask.to(device)\n",
    "            \n",
    "            sarc_y = sarc_y.to(device)\n",
    "            sent_y = sent_y.to(device)\n",
    "            stance_y = stance_y.to(device)\n",
    "            sim_y = sim_y.to(device)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            sarcasm_logits, sentiment_logits, similarity_logits, stance_logits = model(mt_input_ids, mt_attention_mask, ct_input_ids, ct_attention_mask)\n",
    "            \n",
    "            sarcasm_loss = ce_loss(sarcasm_logits, sarc_y)\n",
    "            sentiment_loss = ce_loss(sentiment_logits, sent_y)\n",
    "            similarity_loss = mse_loss(similarity_logits.squeeze(), sim_y.float())\n",
    "            stance_loss = ce_loss(stance_logits, stance_y)\n",
    "\n",
    "            total_loss = sarcasm_loss + sentiment_loss + similarity_loss + stance_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            correct_sarcasm += (sarcasm_logits.argmax(dim=1) == sarc_y).sum().item()\n",
    "            correct_sentiment += (sentiment_logits.argmax(dim=1) == sent_y).sum().item()\n",
    "            correct_stance += (stance_logits.argmax(dim=1) == stance_y).sum().item()\n",
    "            correct_similarity += ((similarity_logits.squeeze() > 0.5).float() == sim_y.float()).sum().item()\n",
    "            total_samples += mt_input_ids.size(0)\n",
    "            \n",
    "            epoch_sarcasm_loss += sarcasm_loss.item()\n",
    "            epoch_sentiment_loss += sentiment_loss.item()\n",
    "            epoch_stance_loss += stance_loss.item()\n",
    "            epoch_similarity_loss += similarity_loss.item()\n",
    "           \n",
    "    avg_sarcasm_loss = epoch_sarcasm_loss / total_samples\n",
    "    avg_sentiment_loss = epoch_sentiment_loss / total_samples\n",
    "    avg_stance_loss = epoch_stance_loss / total_samples\n",
    "    avg_similarity_loss = epoch_similarity_loss / total_samples\n",
    "\n",
    "    sarcasm_acc = correct_sarcasm / total_samples\n",
    "    sentiment_acc = correct_sentiment / total_samples\n",
    "    stance_acc = correct_stance / total_samples\n",
    "    similarity_acc = correct_similarity / total_samples\n",
    "\n",
    "    print(\"Training Results:\")\n",
    "    print(f\"Sarcasm -> Loss: {avg_sarcasm_loss}, Acc: {sarcasm_acc}\")\n",
    "    print(f\"Sentiment -> Loss: {avg_sentiment_loss}, Acc: {sentiment_acc}\")\n",
    "    print(f\"Stance -> Loss: {avg_stance_loss}, Acc: {stance_acc}\")\n",
    "    print(f\"Similarity -> Loss: {avg_similarity_loss}\" )#, Acc: {similarity_acc}\")\n",
    "    \n",
    "    \n",
    "    # VALIDATION -------------------------------------------------------------------------------\n",
    "    model.eval()\n",
    "    valid_sarcasm_loss = 0.0\n",
    "    valid_sentiment_loss = 0.0\n",
    "    valid_similarity_loss = 0.0\n",
    "    valid_stance_loss = 0.0\n",
    "    \n",
    "    valid_correct_sarcasm = 0\n",
    "    valid_correct_sentiment = 0\n",
    "    valid_correct_similarity = 0\n",
    "    valid_correct_stance = 0\n",
    "    valid_total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for main_tweets, context_tweets, sent_y, sarc_y, stance_y, sim_y in tqdm(valid_loader_gen, desc=\"Validation\"):\n",
    "            dataset = TensorDataset(\n",
    "                torch.cat([item[\"input_ids\"] for item in main_tweets]),\n",
    "                torch.cat([item[\"attention_mask\"] for item in main_tweets]),\n",
    "                torch.cat([item[\"input_ids\"] for item in context_tweets]),\n",
    "                torch.cat([item[\"attention_mask\"] for item in context_tweets]),\n",
    "                torch.tensor(sarc_y),\n",
    "                torch.tensor(sent_y),\n",
    "                torch.tensor(stance_y),\n",
    "                torch.tensor(sim_y),\n",
    "            )\n",
    "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            for batch in loader:\n",
    "                mt_input_ids, mt_attention_mask, ct_input_ids, ct_attention_mask, sarc_y, sent_y, stance_y, sim_y = batch\n",
    "                mt_input_ids = mt_input_ids.to(device)\n",
    "                mt_attention_mask = mt_attention_mask.to(device)\n",
    "                ct_input_ids = ct_input_ids.to(device)\n",
    "                ct_attention_mask = ct_attention_mask.to(device)\n",
    "\n",
    "                sarc_y = sarc_y.to(device)\n",
    "                sent_y = sent_y.to(device)\n",
    "                stance_y = stance_y.to(device)\n",
    "                sim_y = sim_y.to(device)\n",
    "\n",
    "                sarcasm_logits, sentiment_logits, similarity_logits, stance_logits = model(mt_input_ids, mt_attention_mask, ct_input_ids, ct_attention_mask)\n",
    "\n",
    "                sarcasm_loss = ce_loss(sarcasm_logits, sarc_y)\n",
    "                sentiment_loss = ce_loss(sentiment_logits, sent_y)\n",
    "                similarity_loss = mse_loss(similarity_logits.squeeze(), sim_y.float())\n",
    "                stance_loss = ce_loss(stance_logits, stance_y)\n",
    "\n",
    "                valid_sarcasm_loss += sarcasm_loss.item()\n",
    "                valid_sentiment_loss += sentiment_loss.item()\n",
    "                valid_similarity_loss += similarity_loss.item()\n",
    "                valid_stance_loss += stance_loss.item()\n",
    "\n",
    "                valid_correct_sarcasm += (sarcasm_logits.argmax(dim=1) == sarc_y).sum().item()\n",
    "                valid_correct_sentiment += (sentiment_logits.argmax(dim=1) == sent_y).sum().item()\n",
    "                valid_correct_similarity += ((similarity_logits.squeeze() > 0.5).float() == sim_y.float()).sum().item()\n",
    "                valid_correct_stance += ((stance_logits.squeeze() > 0.5).float() == stance_y.float()).sum().item()\n",
    "                valid_total_samples += mt_input_ids.size(0)\n",
    "    \n",
    "    avg_valid_sarcasm_loss = valid_sarcasm_loss / valid_total_samples\n",
    "    avg_valid_sentiment_loss = valid_sentiment_loss / valid_total_samples\n",
    "    avg_valid_similarity_loss = valid_similarity_loss / valid_total_samples\n",
    "    avg_valid_stance_loss = valid_stance_loss / valid_total_samples\n",
    "    \n",
    "    valid_sarcasm_acc = valid_correct_sarcasm / valid_total_samples\n",
    "    valid_sentiment_acc = valid_correct_sentiment / valid_total_samples\n",
    "    valid_similarity_acc = valid_correct_similarity / valid_total_samples\n",
    "    valid_stance_acc = valid_correct_stance / valid_total_samples\n",
    "\n",
    "    print(\"Validation Results:\")\n",
    "    print(f\"Sarcasm -> Loss: {avg_valid_sarcasm_loss}, Acc: {valid_sarcasm_acc}\")\n",
    "    print(f\"Sentiment -> Loss: {avg_valid_sentiment_loss}, Acc: {valid_sentiment_acc}\")\n",
    "    print(f\"Stance -> Loss: {avg_valid_stance_loss}, Acc: {valid_stance_acc}\")\n",
    "    print(f\"Similarity -> Loss: {avg_valid_similarity_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbc2cf-411c-4ca3-ad2e-2e287a8bfb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_tweet = \"أنا أؤيد قرار الحكومة الجديدة\"\n",
    "# new_tweet = process_tweet(new_tweet)\n",
    "\n",
    "# main_tweet_embed1 = torch.randn(1, hidden_size_bert)\n",
    "# context_tweet_embed1 = torch.randn(1, hidden_size_bert)\n",
    "# main_tweet_embed2 = torch.randn(1, hidden_size_bert)\n",
    "# context_tweet_embed2 = torch.randn(1, hidden_size_bert)\n",
    "# inputs = [\n",
    "#     [main_tweet_embed1, context_tweet_embed1],\n",
    "#     [main_tweet_embed2, context_tweet_embed2]\n",
    "# ]\n",
    "\n",
    "# # Forward pass through the model\n",
    "# similarities = model(inputs)\n",
    "# # sentiment_logits, sarcasm_logits, similarities = model(inputs)\n",
    "\n",
    "# # Print the outputs\n",
    "# # print(\"Sentiment Logits:\", sentiment_logits)\n",
    "# # print(\"Sarcasm Logits:\", sarcasm_logits)\n",
    "# print(\"Similarities:\", similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346cf3e-46e0-4bc3-83f7-265b5992415c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
