{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b46c978-1d90-488c-87a6-4c4aafca3258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AdamW\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8122340-3e18-4027-90c7-dc4e997b1029",
   "metadata": {},
   "source": [
    "<h2 style='color:red'>NOTE: change all occurrences of MPS to CUDA if not training on macOS</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c6f73-5b10-4f85-8e87-e79746c3ef08",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "197434b1-87c5-4482-af6c-78f257b61653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/gufran/Developer/Projects/AI/MawqifStanceDetection/data\"\n",
    "MODEL_PATH = \"/Users/gufran/Developer/Projects/AI/MawqifStanceDetection/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534e08fc-6def-4724-ad44-758de0fa25d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target = \"all\"\n",
    "#target = 'Covid Vaccine'\n",
    "#target = 'Digital Transformation'\n",
    "#target = 'Women empowerment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c836a58-e532-4caf-aad8-16b3cc2b0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_combination_method = \"average\"\n",
    "# ensemble_combination_method = \"concatenate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dedacaef-76a4-498b-b55f-915afa9bef5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"aubmindlab/bert-base-arabertv02-twitter\"\n",
    "# \"aubmindlab/bert-base-arabertv02\"\n",
    "# \"UBC-NLP/MARBERT\"\n",
    "# \"CAMeL-Lab/bert-base-arabic-camelbert-da\"\n",
    "\n",
    "bert_models = [\n",
    "    \"aubmindlab/bert-base-arabertv02-twitter\", \n",
    "    \"aubmindlab/bert-base-arabertv02\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9340d89f-dfc3-412c-8e66-2106a254e546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 1e-5\n",
    "dropout = 0.1\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9654aca4-26e6-45a8-bf9b-3649569514b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#FOR MAC --------------------------------------------\n",
    "torch.mps.manual_seed(seed)\n",
    "torch.backends.mps.deterministic=True\n",
    "torch.backends.mps.benchmark = False\n",
    "\n",
    "#FOR WINDOWS AND LINUX -------------------------------\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)\n",
    "# torch.backends.cudnn.deterministic=True #replace mps with cudnn here\n",
    "# torch.backends.cudnn.benchmark = False #replace mps with cudnn here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4784b50c-ee68-4d5d-bdec-76979a5b6b43",
   "metadata": {},
   "source": [
    "## DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee9b48ab-6b6a-4452-aba8-d8e7681f157a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>والوزارة كل يوم مغردين عن التحول الرقمي والت...</td>\n",
       "      <td>No</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Favor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تمكين المرأة يعني فتح المجال لها للعمل في مجال...</td>\n",
       "      <td>No</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Favor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MENTION تمكين المرأة السعودية ورحلة كفاحها هو ...</td>\n",
       "      <td>No</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Favor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>هذا الدراسه العالميه الحديثه التي تعاون و شارك...</td>\n",
       "      <td>No</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Favor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>نحن نتعرض لحملة موافقة اجبارية ممنهجة😅 ماتلاحظ...</td>\n",
       "      <td>No</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Against</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sarcasm sentiment  \\\n",
       "0    والوزارة كل يوم مغردين عن التحول الرقمي والت...      No   Neutral   \n",
       "1  تمكين المرأة يعني فتح المجال لها للعمل في مجال...      No   Neutral   \n",
       "2  MENTION تمكين المرأة السعودية ورحلة كفاحها هو ...      No  Positive   \n",
       "3  هذا الدراسه العالميه الحديثه التي تعاون و شارك...      No   Neutral   \n",
       "4  نحن نتعرض لحملة موافقة اجبارية ممنهجة😅 ماتلاحظ...      No   Neutral   \n",
       "\n",
       "    stance  \n",
       "0    Favor  \n",
       "1    Favor  \n",
       "2    Favor  \n",
       "3    Favor  \n",
       "4  Against  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"Mawqif_AllTargets_Train.csv\"))\n",
    "df = df.dropna(subset=[\"stance\"])\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df = df[[\"text\", \"sarcasm\", \"sentiment\", \"stance\"]]\n",
    "\n",
    "if target != \"all\":\n",
    "    df = df[df.target == target]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1a9a0e9-9122-454a-b4e7-2087af28eca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapping_sarcasm = {\"No\": 0, \"Yes\": 1}\n",
    "mapping_stance = {\"Favor\": 1, \"Against\": 0}\n",
    "mapping_sentiment = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "\n",
    "df['sarcasm'] = df['sarcasm'].map(lambda x: mapping_sarcasm[x])\n",
    "df['sentiment'] = df['sentiment'].map(lambda x: mapping_sentiment[x])\n",
    "df['stance'] = df['stance'].map(lambda x: mapping_stance[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "829e4180-e0d2-4ea3-a3a6-43a76cb04eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_punctuations = '''`÷×؛<>()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "def remove_hash_URL_MEN(text):\n",
    "    text = re.sub(r'#',' ',text)\n",
    "    text = re.sub(r'_',' ',text)\n",
    "    text = re.sub(r'URL','',text)\n",
    "    text = re.sub(r'MENTION','',text)\n",
    "    return text\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إآ]\", \"ا\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def process_tweet(tweet):     \n",
    "    tweet=remove_hash_URL_MEN(tweet)\n",
    "    tweet = re.sub('@[^\\s]+', ' ', str(tweet))\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',str(tweet))    \n",
    "    tweet= normalize_arabic(str(tweet))\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "arabert_prep = ArabertPreprocessor(model_name=\"aubmindlab/bert-base-arabertv02-twitter\")\n",
    "df.text = df.text.apply(lambda x: process_tweet(x))\n",
    "df.text = df.text.apply(lambda x: arabert_prep.preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ab11eb2-8320-45df-9eb7-7dc30774af85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>والوزارة كل يوم مغردين عن التحول الرقمي والتعل...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تمكين المرأة يعني فتح المجال لها للعمل في مجال...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تمكين المرأة السعودية ورحلة كفاحها هو الانتصار</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>هذا الدراسه العالميه الحديثه التي تعاون و شارك...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>نحن نتعرض لحملة موافقة اجبارية ممنهجة 😅 ماتلاح...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sarcasm  sentiment  \\\n",
       "0  والوزارة كل يوم مغردين عن التحول الرقمي والتعل...        0          1   \n",
       "1  تمكين المرأة يعني فتح المجال لها للعمل في مجال...        0          1   \n",
       "2     تمكين المرأة السعودية ورحلة كفاحها هو الانتصار        0          2   \n",
       "3  هذا الدراسه العالميه الحديثه التي تعاون و شارك...        0          1   \n",
       "4  نحن نتعرض لحملة موافقة اجبارية ممنهجة 😅 ماتلاح...        0          1   \n",
       "\n",
       "   stance  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0c3be2d-9402-42ca-8df2-1a0d0d8076e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df[[\"text\"]]\n",
    "y = df[[\"sarcasm\", \"sentiment\", \"stance\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e8f5e7d-6b5b-42b1-9355-fd465d3f4e84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2693, 1) y_train shape: (2693, 3) X_test shape: (476, 1) y_test shape: (476, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape, \"X_test shape:\", X_test.shape, \"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1c209bc-c448-4e01-8e90-6797a90e4fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_text(text, tokenizer):\n",
    "    return tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "encoded_tweets_train = []\n",
    "encoded_tweets_test = []\n",
    "for bert_model in bert_models:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
    "    encoded_tweets_train.append([encode_text(text, tokenizer) for text in X_train[\"text\"]])\n",
    "    encoded_tweets_test.append([encode_text(text, tokenizer) for text in X_test[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9907e689-c3fd-4ab7-a94a-4a12901fb062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(encoded_tweets, labels):    \n",
    "    tweets, sentiments, sarcasms, stances = [], [], [], []\n",
    "    \n",
    "    for i in range(len(encoded_tweets)):\n",
    "        model_specific_tweets = []\n",
    "        for j in range(len(encoded_tweets[i])):\n",
    "            model_specific_tweets.append(encoded_tweets[i][j])            \n",
    "        tweets.append(model_specific_tweets)\n",
    "        \n",
    "    for i in range(len(labels)):\n",
    "        sentiments.append(labels.sentiment.iloc[i])\n",
    "        sarcasms.append(labels.sarcasm.iloc[i])\n",
    "        stances.append(labels.stance.iloc[i])\n",
    "            \n",
    "    return tweets, sentiments, sarcasms, stances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a1409f3-1c40-46ec-af6e-f77898147f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets, sentiments, sarcasms, stances = data_generator(encoded_tweets_train, y_train)\n",
    "train_dataset = TensorDataset(\n",
    "\n",
    "    *[torch.cat([item[\"input_ids\"] for item in enc_tweets]) for enc_tweets in tweets],\n",
    "    *[torch.cat([item[\"attention_mask\"] for item in enc_tweets]) for enc_tweets in tweets],\n",
    "    torch.tensor(sarcasms),\n",
    "    torch.tensor(sentiments),\n",
    "    torch.tensor(stances),\n",
    "\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "tweets, sentiments, sarcasms, stances = data_generator(encoded_tweets_test, y_test)\n",
    "val_dataset = TensorDataset(\n",
    "    *[\n",
    "        *[torch.cat([item[\"input_ids\"] for item in enc_tweets]) for enc_tweets in tweets],\n",
    "        *[torch.cat([item[\"attention_mask\"] for item in enc_tweets]) for enc_tweets in tweets],\n",
    "        torch.tensor(sarcasms),\n",
    "        torch.tensor(sentiments),\n",
    "        torch.tensor(stances),\n",
    "    ]\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61f5bcbf-eb8b-4d64-8431-8b7faff417db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sarcasm_labels = len(df.sarcasm.unique())\n",
    "num_sentiment_labels = len(df.sentiment.unique())\n",
    "num_stance_labels = len(df.stance.unique())\n",
    "\n",
    "num_sarcasm_labels, num_sentiment_labels, num_stance_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987a76c-5bc9-4967-ad1e-4525d02515ea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8436701-67d5-40f9-bf93-ebc3290befc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fb3f629-18d4-4bbb-b6d4-adb7fbdcd1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TaskHead(nn.Module):\n",
    "    def __init__(self, num_labels, hidden_size):\n",
    "        super(TaskHead, self).__init__()\n",
    "        \n",
    "        self.num_labels = num_labels\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        logits = F.dropout(inputs, p=dropout, training=self.training)\n",
    "        logits = self.classifier(logits)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        self.classifier.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        if self.classifier.bias is not None:\n",
    "            self.classifier.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c772d61-1c37-47ff-812f-2272fefe2be7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, bert_models, sentiment_head, sarcasm_head, stance_head, subtask_hidden_layer_size, combination_method):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        \n",
    "        self.combination_method = combination_method\n",
    "        self.bert_models = nn.ModuleList([AutoModel.from_pretrained(bert, force_download=True, resume_download=False).to(device) for bert in bert_models])\n",
    "                        \n",
    "        self.hidden_layer = nn.Linear(self.get_hidden_in(), subtask_hidden_layer_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.sentiment_head = sentiment_head\n",
    "        self.sarcasm_head = sarcasm_head\n",
    "        self.stance_head = stance_head\n",
    "                \n",
    "    def get_hidden_in(self):\n",
    "        if len(self.bert_models) == 1:\n",
    "            return self.bert_models[0].config.hidden_size\n",
    "        \n",
    "        if self.combination_method == \"average\":\n",
    "            return self.bert_models[0].config.hidden_size\n",
    "        \n",
    "        if self.combination_method == \"concatenate\":\n",
    "            return len(self.bert_models) * self.bert_models[0].config.hidden_size\n",
    "        \n",
    "    def concatenate(self, bert_outputs):\n",
    "        return torch.cat(bert_outputs, dim=-1)\n",
    "\n",
    "    def average(self, bert_outputs):\n",
    "        return torch.mean(torch.stack(bert_outputs), dim=0)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = []\n",
    "        \n",
    "        for i, bert in enumerate(self.bert_models):\n",
    "            outputs.append(bert(input_ids=input_ids[i], attention_mask=attention_mask[i]).last_hidden_state[:, 0, :])\n",
    "\n",
    "        combined_emb = outputs[0]\n",
    "        if len(self.bert_models) > 1:\n",
    "            if self.combination_method == \"average\":\n",
    "                combined_emb = self.average(outputs)\n",
    "            if self.combination_method == \"concatenate\":\n",
    "                combined_emb = self.concatenate(outputs)\n",
    "        \n",
    "        hidden_output = self.dropout(F.relu(self.hidden_layer(combined_emb)))\n",
    "        sarcasm_logits = self.sarcasm_head(hidden_output)\n",
    "        sentiment_logits = self.sentiment_head(hidden_output)\n",
    "        stance_logits = self.stance_head(hidden_output)\n",
    "        \n",
    "        return sarcasm_logits, sentiment_logits, stance_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e077a-c20c-40b1-b374-a71249e27cf6",
   "metadata": {},
   "source": [
    "## TRAINING & VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4524f0a2-78dd-4f07-9fc7-db364afa7003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearDecayLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, n_epoch, start_decay, last_epoch=-1):\n",
    "        self.start_decay=start_decay\n",
    "        self.n_epoch=n_epoch\n",
    "        super(LinearDecayLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        last_epoch = self.last_epoch\n",
    "        n_epoch=self.n_epoch\n",
    "        b_lr=self.base_lrs[0]\n",
    "        start_decay=self.start_decay\n",
    "        if last_epoch>start_decay:\n",
    "            lr=b_lr-b_lr/(n_epoch-start_decay)*(last_epoch-start_decay)\n",
    "        else:\n",
    "            lr=b_lr\n",
    "        return [lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "183394a0-43d4-40f0-b3e4-cfdfc13a130d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02-twitter and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19b713af47a47eaab4815391b4ab548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/543M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Consistency check failed: file should be of size 543432324 but has size 169318876 (model.safetensors).\nWe are sorry for the inconvenience. Please retry download and pass `force_download=True, resume_download=False` as argument.\nIf the issue persists, please let us know by opening an issue on https://github.com/huggingface/huggingface_hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m hidden_layer_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m MultiTaskModel(\n\u001b[1;32m      3\u001b[0m     bert_models,\n\u001b[1;32m      4\u001b[0m     TaskHead(num_sentiment_labels, hidden_layer_size),\n\u001b[1;32m      5\u001b[0m     TaskHead(num_sarcasm_labels, hidden_layer_size),\n\u001b[1;32m      6\u001b[0m     TaskHead(num_stance_labels, hidden_layer_size),\n\u001b[1;32m      7\u001b[0m     hidden_layer_size,\n\u001b[1;32m      8\u001b[0m     ensemble_combination_method,\n\u001b[1;32m      9\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m     12\u001b[0m ce_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36mMultiTaskModel.__init__\u001b[0;34m(self, bert_models, sentiment_head, sarcasm_head, stance_head, subtask_hidden_layer_size, combination_method)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(MultiTaskModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombination_method \u001b[38;5;241m=\u001b[39m combination_method\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_models \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(bert)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m bert \u001b[38;5;129;01min\u001b[39;00m bert_models])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_hidden_in(), subtask_hidden_layer_size)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(dropout)\n",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(MultiTaskModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombination_method \u001b[38;5;241m=\u001b[39m combination_method\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_models \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(bert)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m bert \u001b[38;5;129;01min\u001b[39;00m bert_models])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_hidden_in(), subtask_hidden_layer_size)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(dropout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:516\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    515\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    517\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:2695\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2680\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2683\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   2684\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   2694\u001b[0m     }\n\u001b[0;32m-> 2695\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs)\n\u001b[1;32m   2697\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   2698\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   2699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   2700\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:428\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    429\u001b[0m         path_or_repo_id,\n\u001b[1;32m    430\u001b[0m         filename,\n\u001b[1;32m    431\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    432\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    433\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    434\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    435\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    436\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    437\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    438\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    439\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    440\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_file_manager() \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[1;32m   1362\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, temp_file\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m-> 1364\u001b[0m     http_get(\n\u001b[1;32m   1365\u001b[0m         url_to_download,\n\u001b[1;32m   1366\u001b[0m         temp_file,\n\u001b[1;32m   1367\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1368\u001b[0m         resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[1;32m   1369\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1370\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:547\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    544\u001b[0m         temp_file\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m expected_size \u001b[38;5;241m!=\u001b[39m temp_file\u001b[38;5;241m.\u001b[39mtell():\n\u001b[0;32m--> 547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsistency check failed: file should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but has size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_file\u001b[38;5;241m.\u001b[39mtell()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWe are sorry for the inconvenience. Please retry download and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pass `force_download=True, resume_download=False` as argument.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf the issue persists, please let us\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m know by opening an issue on https://github.com/huggingface/huggingface_hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    554\u001b[0m progress\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mOSError\u001b[0m: Consistency check failed: file should be of size 543432324 but has size 169318876 (model.safetensors).\nWe are sorry for the inconvenience. Please retry download and pass `force_download=True, resume_download=False` as argument.\nIf the issue persists, please let us know by opening an issue on https://github.com/huggingface/huggingface_hub."
     ]
    }
   ],
   "source": [
    "hidden_layer_size = 256\n",
    "model = MultiTaskModel(\n",
    "    bert_models,\n",
    "    TaskHead(num_sentiment_labels, hidden_layer_size),\n",
    "    TaskHead(num_sarcasm_labels, hidden_layer_size),\n",
    "    TaskHead(num_stance_labels, hidden_layer_size),\n",
    "    hidden_layer_size,\n",
    "    ensemble_combination_method,\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "lr_scheduler=LinearDecayLR(optimizer, num_epochs, int(num_epochs*0.5))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] || Learning Rate: {lr_scheduler.get_lr()}\")\n",
    "    \n",
    "    # TRAINING -------------------------------------------------------------------------------\n",
    "    model.train()\n",
    "    train_loader_gen = data_generator(encoded_tweets_train, y_train)\n",
    "    valid_loader_gen = data_generator(encoded_tweets_test, y_test)\n",
    "    \n",
    "    train_sarcasm_loss, train_sentiment_loss, train_stance_loss = 0.0, 0.0, 0.0    \n",
    "    correct_sarcasm, correct_sentiment, correct_stance, total_samples = 0,0,0,0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        *tweet_data, sarc_y, sent_y, stance_y = batch\n",
    "        \n",
    "        input_ids, attention_mask = [], []\n",
    "        for i in range(len(tweet_data)//2):\n",
    "            input_ids.append(tweet_data[i])\n",
    "        for i in range(len(tweet_data)//2,len(tweet_data)):\n",
    "            attention_mask.append(tweet_data[i])\n",
    "        \n",
    "        for i in range(len(input_ids)):\n",
    "            input_ids[i] = input_ids[i].to(device)\n",
    "            attention_mask[i] = attention_mask[i].to(device)\n",
    "        sarc_y = sarc_y.to(device)\n",
    "        sent_y = sent_y.to(device)\n",
    "        stance_y = stance_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        sarcasm_logits, sentiment_logits, stance_logits = model(input_ids, attention_mask)\n",
    "\n",
    "        sarcasm_loss = ce_loss(sarcasm_logits, sarc_y)\n",
    "        sentiment_loss = ce_loss(sentiment_logits, sent_y)\n",
    "        stance_loss = ce_loss(stance_logits, stance_y)\n",
    "\n",
    "        total_loss = sarcasm_loss + sentiment_loss + stance_loss\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        correct_sarcasm += (sarcasm_logits.argmax(dim=1) == sarc_y).sum().item()\n",
    "        correct_sentiment += (sentiment_logits.argmax(dim=1) == sent_y).sum().item()\n",
    "        correct_stance += (stance_logits.argmax(dim=1) == stance_y).sum().item()\n",
    "        total_samples += input_ids[0].size(0)\n",
    "        \n",
    "        train_sarcasm_loss += sarcasm_loss.item()\n",
    "        train_sentiment_loss += sentiment_loss.item()\n",
    "        train_stance_loss += stance_loss.item()\n",
    "           \n",
    "    avg_sarcasm_loss = train_sarcasm_loss / total_samples\n",
    "    avg_sentiment_loss = train_sentiment_loss / total_samples\n",
    "    avg_stance_loss = train_stance_loss / total_samples\n",
    "\n",
    "    sarcasm_acc = correct_sarcasm / total_samples\n",
    "    sentiment_acc = correct_sentiment / total_samples\n",
    "    stance_acc = correct_stance / total_samples\n",
    "\n",
    "    print(f\"Sarcasm -> Loss: {avg_sarcasm_loss:.4f}, Acc: {sarcasm_acc:.4f}\")\n",
    "    print(f\"Sentiment -> Loss: {avg_sentiment_loss:.4f}, Acc: {sentiment_acc:.4f}\")\n",
    "    print(f\"Stance -> Loss: {avg_stance_loss:.4f}, Acc: {stance_acc:.4f}\")\n",
    "    \n",
    "    \n",
    "    # VALIDATION -------------------------------------------------------------------------------\n",
    "    model.eval()\n",
    "    valid_sarcasm_loss, valid_sentiment_loss, valid_stance_loss = 0.0, 0.0, 0.0\n",
    "    valid_correct_sarcasm, valid_correct_sentiment, valid_correct_stance, valid_total_samples = 0,0,0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            *tweet_data, sarc_y, sent_y, stance_y = batch\n",
    "\n",
    "            input_ids, attention_mask = [], []\n",
    "            for i in range(len(tweet_data)//2):\n",
    "                input_ids.append(tweet_data[i])\n",
    "            for i in range(len(tweet_data)//2,len(tweet_data)):\n",
    "                attention_mask.append(tweet_data[i])\n",
    "\n",
    "            for i in range(len(input_ids)):\n",
    "                input_ids[i] = input_ids[i].to(device)\n",
    "                attention_mask[i] = attention_mask[i].to(device)\n",
    "            sarc_y = sarc_y.to(device)\n",
    "            sent_y = sent_y.to(device)\n",
    "            stance_y = stance_y.to(device)\n",
    "\n",
    "            sarcasm_logits, sentiment_logits, stance_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            sarcasm_loss = ce_loss(sarcasm_logits, sarc_y)\n",
    "            sentiment_loss = ce_loss(sentiment_logits, sent_y)\n",
    "            stance_loss = ce_loss(stance_logits, stance_y)\n",
    "\n",
    "            valid_sarcasm_loss += sarcasm_loss.item()\n",
    "            valid_sentiment_loss += sentiment_loss.item()\n",
    "            valid_stance_loss += stance_loss.item()\n",
    "\n",
    "            valid_correct_sarcasm += (sarcasm_logits.argmax(dim=1) == sarc_y).sum().item()\n",
    "            valid_correct_sentiment += (sentiment_logits.argmax(dim=1) == sent_y).sum().item()\n",
    "            valid_correct_stance += (stance_logits.argmax(dim=1) == stance_y).sum().item()\n",
    "            valid_total_samples += input_ids[0].size(0)\n",
    "    \n",
    "    avg_valid_sarcasm_loss = valid_sarcasm_loss / valid_total_samples\n",
    "    avg_valid_sentiment_loss = valid_sentiment_loss / valid_total_samples\n",
    "    avg_valid_stance_loss = valid_stance_loss / valid_total_samples\n",
    "    \n",
    "    valid_sarcasm_acc = valid_correct_sarcasm / valid_total_samples\n",
    "    valid_sentiment_acc = valid_correct_sentiment / valid_total_samples\n",
    "    valid_stance_acc = valid_correct_stance / valid_total_samples\n",
    "\n",
    "    print(f\"Sarcasm -> Loss: {avg_valid_sarcasm_loss:.4f}, Acc: {valid_sarcasm_acc:.4f}\")\n",
    "    print(f\"Sentiment -> Loss: {avg_valid_sentiment_loss:.4f}, Acc: {valid_sentiment_acc:.4f}\")\n",
    "    print(f\"Stance -> Loss: {avg_valid_stance_loss:.4f}, Acc: {valid_stance_acc:.4f}\\n\\n\")\n",
    "    \n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc100b7-9cb5-47de-a4d1-0b08cc0d40f7",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b77c28-a9a1-4c01-b741-d9fc0c6d2016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
