{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AdamW\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mawqif_AllTargets_Test.csv',\n",
       " '.DS_Store',\n",
       " 'Mawqif_AllTargets_Blind Test.csv',\n",
       " 'Mawqif_AllTargets_Train.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"../../data\"\n",
    "os.listdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(os.path.join(DATA_PATH, \"Mawqif_AllTargets_Test.csv\"))\n",
    "df_test[\"stance\"].replace({np.nan: \"None\"}, inplace=True)\n",
    "\n",
    "with open(f\"../../data/gold.txt\", \"w\") as f:\n",
    "    for index, row in df_test.iterrows():\n",
    "        f.write(f\"{row['ID']}\\t{row['target']}\\t{row['text']}\\t{row['stance']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_models(indices):\n",
    "    bert_models = [\n",
    "        \"aubmindlab/bert-base-arabertv02-twitter\", \n",
    "        \"aubmindlab/bert-base-arabertv02\",\n",
    "        \"UBC-NLP/MARBERT\",\n",
    "        \"CAMeL-Lab/bert-base-arabic-camelbert-da\"\n",
    "    ]\n",
    "    return [bert_models[i] for i in indices]\n",
    "\n",
    "ensemble_settings = {\n",
    "    \"ENSEMBLE_AVERAGE\" : 0,\n",
    "    \"ENSEMBLE_ATTENTION\" : 1,\n",
    "    \"ENSEMBLE_GRAPH\" : 2\n",
    "}\n",
    "\n",
    "weighting_settings = {\n",
    "    \"EQUAL_WEIGHTING\" : 0,\n",
    "    \"STATIC_WEIGHTING\" : 1,\n",
    "    \"RELATIVE_WEIGHTING\" : 2,\n",
    "    \"HEIRARCHICAL_WEIGHTING\" : 3,\n",
    "    \"PRIORITIZE_HIGH_CONFIDENCE_WEIGHTING\" : 4,\n",
    "    \"PRIORITIZE_LOW_CONFIDENCE_WEIGHTING\" : 5,\n",
    "    \"META_WEIGHTING\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = os.listdir(\"../../res\")\n",
    "\n",
    "c = 0\n",
    "for pred in preds:\n",
    "    if \"pool\" not in pred or \"txt\" in pred:\n",
    "        continue\n",
    "    \n",
    "    # c+=1\n",
    "    # model_path = pred\n",
    "    \n",
    "    # pool_arg_index = model_path.index(\"pool\")\n",
    "    # end_index = model_path[pool_arg_index:].index(\"_\")\n",
    "    # pooling = model_path[pool_arg_index+5:pool_arg_index + end_index]\n",
    "\n",
    "    # use_bi_arg_index = model_path.index(\"bi\")\n",
    "    # end_index = model_path[use_bi_arg_index:].index(\"_\")\n",
    "    # use_bi = model_path[use_bi_arg_index+3:use_bi_arg_index + end_index]\n",
    "\n",
    "    # use_gru_arg_index = model_path.index(\"gru\")\n",
    "    # end_index = model_path[use_gru_arg_index:].index(\"_\")\n",
    "    # use_gru = model_path[use_gru_arg_index+4:use_gru_arg_index + end_index]\n",
    "\n",
    "    # bert_arg_index = model_path.index(\"bert\")\n",
    "    # end_index = model_path[bert_arg_index:].index(\"_\")\n",
    "    # bert_model_indices = model_path[bert_arg_index+6:bert_arg_index + end_index]\n",
    "\n",
    "    # ensemble_setting_arg_index = model_path.index(\"ENSEMBLE_\")\n",
    "    # end_index = model_path[ensemble_setting_arg_index + len(\"ENSEMBLE_\"):].index(\"_\")\n",
    "    # ensemble_setting = model_path[ensemble_setting_arg_index:ensemble_setting_arg_index + len(\"ENSEMBLE_\") + end_index]\n",
    "    # ensemble_setting = str(ensemble_settings[ensemble_setting])\n",
    "\n",
    "    # ws = \"EQUAL_\" if \"EQUAL\" in model_path else \"\"\n",
    "    # ws = \"STATIC_\" if \"STATIC\" in model_path else ws\n",
    "    # ws = \"RELATIVE_\" if \"RELATIVE\" in model_path else ws\n",
    "    # ws = \"HEIRARCHICAL_\" if \"HEIRARCHICAL\" in model_path else ws\n",
    "    # weighting_setting_arg_index = model_path.index(ws)\n",
    "    # end_index = model_path[weighting_setting_arg_index + len(ws):].index(\"_\")\n",
    "    # weighting_setting = model_path[weighting_setting_arg_index:weighting_setting_arg_index + len(ws) + end_index]\n",
    "    # weighting_setting = str(weighting_settings[weighting_setting])\n",
    "\n",
    "    # bert_models = get_bert_models([int(index) for index in list(bert_model_indices)])\n",
    "\n",
    "    # print(\"MODEL SETTINGS\")\n",
    "    # print(f\"Model Path: {model_path}\")\n",
    "    # print(f\"Bert Models: {bert_models}\")\n",
    "    # print(f\"Pooling: {pooling}\")\n",
    "    # print(f\"Use Bi: {use_bi}\")\n",
    "    # print(f\"Use GRU: {use_gru}\")\n",
    "    # print(f\"Ensemble Setting: {ensemble_setting}\")\n",
    "    # print(f\"Weighting Setting: {weighting_setting}\")\n",
    "    # print()\n",
    "\n",
    "    #make a guess file in the following format ID<Tab>Target<Tab>Tweet<Tab>Stance\n",
    "    #save it in the same directory as the model\n",
    "    \n",
    "    df = pd.read_csv(f\"../../res/{pred}\")\n",
    "    df[\"pred\"].replace({\"None\": np.nan}, inplace=True)\n",
    "\n",
    "    with open(f\"../../res//guess_{pred}.txt\", \"w\") as f:\n",
    "        for index, row in df.iterrows():\n",
    "            f.write(f\"{row['ID']}\\t{row['target']}\\t{row['text']}\\t{row['pred']}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
